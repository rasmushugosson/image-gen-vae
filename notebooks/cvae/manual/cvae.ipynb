{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 06:08:29.741047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.16.1\n",
      "Available GPUs:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 06:08:36.764443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:36.813567: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:36.813627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../image_gen_dm')) # or the path to your source code\n",
    "sys.path.append(str(module_path))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "tfkl = tfk.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(TF_ENABLE_ONEDNN_OPTS)\n",
    "os.environ['TG_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import image_gen_vae as igvae\n",
    "import image_gen_vae.constants as consts\n",
    "\n",
    "print('Tensorflow Version:', tf.__version__)\n",
    "print(\"Available GPUs: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "ENCODER_HIDDEN_SIZES = [\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "    [448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [448],\n",
    "    [1792, 448],\n",
    "]\n",
    "\n",
    "DECODER_HIDDEN_SIZES = [] # Construct from encoder hidden sizes in reverse\n",
    "\n",
    "for hidden_sizes in ENCODER_HIDDEN_SIZES:\n",
    "    DECODER_HIDDEN_SIZES.append(hidden_sizes[::-1])\n",
    "\n",
    "\n",
    "ENCODER_CONV_CONFIGS = [\n",
    "    [(16, 3, 2)],\n",
    "    [(16, 3, 2), (32, 3, 2)],\n",
    "    [(16, 3, 2), (32, 3, 2), (64, 3, 2)],\n",
    "\n",
    "    [(32, 5, 4)],\n",
    "    [(32, 5, 4), (64, 5, 2)],\n",
    "    [(32, 5, 4), (64, 3, 2), (128, 3, 2)],\n",
    "\n",
    "    [(32, 3, 2), (64, 3, 2), (128, 3, 2)],\n",
    "    [(32, 3, 2), (64, 3, 2), (128, 3, 2), (256, 3, 2)],\n",
    "    [(64, 3, 2), (128, 5, 4), (256, 3, 2)],\n",
    "    [(64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2)],\n",
    "\n",
    "    [(128, 3, 2), (256, 5, 4), (512, 3, 2), (1024, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 5, 4), (1024, 3, 2), (2048, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2), (2048, 3, 2)],\n",
    "\n",
    "    [(16, 3, 2)],\n",
    "    [(16, 3, 2), (32, 3, 2)],\n",
    "    [(16, 3, 2), (32, 3, 2), (64, 3, 2)],\n",
    "    \n",
    "    [(32, 5, 4)],\n",
    "    [(32, 5, 4), (64, 5, 2)],\n",
    "    [(32, 5, 4), (64, 3, 2), (128, 3, 2)],\n",
    "\n",
    "    [(32, 3, 2), (64, 3, 2), (128, 3, 2)],\n",
    "    [(32, 3, 2), (64, 3, 2), (128, 3, 2), (256, 3, 2)],\n",
    "    [(64, 3, 2), (128, 5, 4), (256, 3, 2)],\n",
    "    [(64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2)],\n",
    "\n",
    "    [(128, 3, 2), (256, 5, 4), (512, 3, 2), (1024, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 5, 4), (1024, 3, 2), (2048, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2), (2048, 3, 2)],\n",
    "\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2), (2048, 3, 2), (4096, 3, 2)],\n",
    "    [(128, 3, 2), (256, 3, 2), (512, 3, 2), (1024, 3, 2), (2048, 3, 2), (4096, 3, 2)],\n",
    "]\n",
    "\n",
    "DECODER_CONV_CONFIGS = [] # Construct from encoder conv configs in reverse\n",
    "\n",
    "for conv_configs in ENCODER_CONV_CONFIGS:\n",
    "    DECODER_CONV_CONFIGS.append(conv_configs[::-1])\n",
    "\n",
    "FLATTENED_SHAPES = [\n",
    "    (32, 32, 16),\n",
    "    (16, 16, 32),\n",
    "    (8, 8, 64),\n",
    "    (16, 16, 32),\n",
    "    (8, 8, 64),\n",
    "    (4, 4, 128),\n",
    "\n",
    "    (8, 8, 128),\n",
    "    (4, 4, 256),\n",
    "    (4, 4, 256),\n",
    "    (4, 4, 512),\n",
    "\n",
    "    (2, 2, 1024),\n",
    "    (4, 4, 1024),\n",
    "    (1, 1, 2048),\n",
    "    (2, 2, 2048),\n",
    "\n",
    "    (32, 32, 16),\n",
    "    (16, 16, 32),\n",
    "    (8, 8, 64),\n",
    "    (16, 16, 32),\n",
    "    (8, 8, 64),\n",
    "    (4, 4, 128),\n",
    "\n",
    "    (8, 8, 128),\n",
    "    (4, 4, 256),\n",
    "    (4, 4, 256),\n",
    "    (4, 4, 512),\n",
    "\n",
    "    (2, 2, 1024),\n",
    "    (4, 4, 1024),\n",
    "    (1, 1, 2048),\n",
    "    (2, 2, 2048),\n",
    "\n",
    "    (1, 1, 4096),\n",
    "    (1, 1, 4096),\n",
    "]\n",
    "\n",
    "FLATTENED_SIZES = [] # Construct from flattened shapes\n",
    "\n",
    "for shape in FLATTENED_SHAPES:\n",
    "    FLATTENED_SIZES.append(shape[0] * shape[1] * shape[2])\n",
    "\n",
    "LATENT_DIM = 2\n",
    "\n",
    "MODEL_NAMES = []\n",
    "\n",
    "for i in range(len(ENCODER_HIDDEN_SIZES)):\n",
    "    if i < 9:\n",
    "        MODEL_NAMES.append(f'cvae0{i+1}')\n",
    "    else:\n",
    "        MODEL_NAMES.append(f'cvae{i+1}')\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "VAL_PERCENTAGE = 0.1\n",
    "\n",
    "# Models to run\n",
    "\n",
    "MODEL_START = 27 # Rasmus 12-21 Viktor 22-29\n",
    "MODEL_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 06:08:36.960899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:36.961031: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:36.961073: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:37.419492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:37.419618: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:37.419635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-29 06:08:37.419689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 06:08:37.419836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images:  8100\n",
      "Evaluation Images:  900\n",
      "Training Images (post-duplication):  8100\n",
      "Validation Images (post-duplication):  900\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "\n",
    "train_ds, val_ds = igvae.utils.load_datasets(val_percentage=VAL_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "\n",
    "class Sampling(tfkl.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CVAE(tfk.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            input_data, target_data = data\n",
    "            z_mean, z_log_var, z = self.encoder(input_data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.losses.binary_crossentropy(target_data, reconstruction), \n",
    "                    axis=(1,2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, input_data, training=False):\n",
    "        _, _, z = self.encoder(input_data)\n",
    "        reconstructed_image = self.decoder(z)\n",
    "\n",
    "        return reconstructed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 28\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 32, 32, 128)          3584      ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 256)          295168    ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 512)            1180160   ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 1024)           4719616   ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 2, 2, 2048)           1887641   ['conv2d_3[0][0]']            \n",
      "                                                          6                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 8192)                 0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1792)                 1468185   ['flatten[0][0]']             \n",
      "                                                          6                                       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 448)                  803264    ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 2)                    898       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 2)                    898       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n",
      "                                                                     'z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40561860 (154.73 MB)\n",
      "Trainable params: 40561860 (154.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 448)               1344      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1792)              804608    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8192)              14688256  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 2048)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 4, 4, 1024)        18875392  \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 8, 8, 512)         4719104   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 16, 16, 256)       1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 32, 128)       295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2D  (None, 64, 64, 3)         3459      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40567107 (154.75 MB)\n",
      "Trainable params: 40567107 (154.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(MODEL_START, MODEL_START + MODEL_COUNT):\n",
    "    print(f'Model {i+1}')\n",
    "\n",
    "    image_input = tfkl.Input(shape=consts.INPUT_SHAPE, name='image_input')\n",
    "    \n",
    "    x = image_input\n",
    "    for filters, kernel_size, strides in ENCODER_CONV_CONFIGS[i]:\n",
    "        x = tfkl.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='relu', padding='same')(x)\n",
    "\n",
    "    flattened_image = tfkl.Flatten()(x)\n",
    "\n",
    "    x = flattened_image\n",
    "    for layer_size in ENCODER_HIDDEN_SIZES[i]:\n",
    "        x = tfkl.Dense(layer_size, \n",
    "                    activation='relu',\n",
    "                    kernel_initializer='glorot_uniform')(x)\n",
    "        \n",
    "    z_mean = tfkl.Dense(LATENT_DIM, name='z_mean', kernel_initializer='glorot_uniform')(x)\n",
    "    z_log_var = tfkl.Dense(LATENT_DIM, name='z_log_var', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    encoder = tfk.Model(inputs=image_input, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    latent_inputs = tfkl.Input(shape=(LATENT_DIM,))\n",
    "\n",
    "    y = latent_inputs\n",
    "    for layer_size in DECODER_HIDDEN_SIZES[i]:\n",
    "        y = tfkl.Dense(layer_size, \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='glorot_uniform')(y)\n",
    "        \n",
    "    reconstructed_flattened_image = tfkl.Dense(FLATTENED_SIZES[i], kernel_initializer='glorot_uniform', activation='relu')(y)\n",
    "    \n",
    "    y = tfkl.Reshape(FLATTENED_SHAPES[i])(reconstructed_flattened_image)\n",
    "   \n",
    "    j = 0\n",
    "    for filters, kernel_size, strides in DECODER_CONV_CONFIGS[i]:\n",
    "        if j == len(DECODER_CONV_CONFIGS[i]) - 1:\n",
    "            y = tfkl.Conv2DTranspose(filters=3, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='sigmoid', padding='same')(y)\n",
    "        else:\n",
    "            y = tfkl.Conv2DTranspose(filters=DECODER_CONV_CONFIGS[i][j + 1][0], kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='relu', padding='same')(y)\n",
    "        j += 1\n",
    "\n",
    "    decoder = tfk.Model(inputs=latent_inputs, outputs=y, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    # Model\n",
    "        \n",
    "    model = CVAE(encoder, decoder)\n",
    "    model.compile(optimizer=tfk.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 06:08:56.446016: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "2024-04-29 06:08:58.563828: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714370939.429281      80 service.cc:145] XLA service 0x7fa28a249b00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714370939.429459      80 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-04-29 06:08:59.446390: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1714370939.547678      80 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/64 [============================>.] - ETA: 0s - loss: 2802.4963 - reconstruction_loss: 2758.2102 - kl_loss: 1.5420"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 06:09:09.892853: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.36GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 34s 194ms/step - loss: 2801.1567 - reconstruction_loss: 2757.3616 - kl_loss: 1.5978 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2692.4822 - reconstruction_loss: 2687.4600 - kl_loss: 3.1018 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2686.2581 - reconstruction_loss: 2678.5923 - kl_loss: 2.6956 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2659.7990 - reconstruction_loss: 2650.8958 - kl_loss: 3.8672 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2636.3940 - reconstruction_loss: 2624.9089 - kl_loss: 4.6073 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2616.8460 - reconstruction_loss: 2610.2910 - kl_loss: 5.0379 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2611.3448 - reconstruction_loss: 2605.7986 - kl_loss: 5.1347 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2609.0855 - reconstruction_loss: 2603.1243 - kl_loss: 5.1546 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2607.0401 - reconstruction_loss: 2601.7209 - kl_loss: 5.1855 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2606.1310 - reconstruction_loss: 2600.0542 - kl_loss: 5.2063 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2604.0036 - reconstruction_loss: 2598.7556 - kl_loss: 5.2580 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2602.7519 - reconstruction_loss: 2597.2058 - kl_loss: 5.3203 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2602.3273 - reconstruction_loss: 2595.5098 - kl_loss: 5.3637 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2599.3860 - reconstruction_loss: 2593.6777 - kl_loss: 5.4894 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2598.2671 - reconstruction_loss: 2592.2439 - kl_loss: 5.5639 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2596.2555 - reconstruction_loss: 2590.6309 - kl_loss: 5.6266 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2594.2649 - reconstruction_loss: 2588.8198 - kl_loss: 5.7568 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2594.8197 - reconstruction_loss: 2588.4338 - kl_loss: 5.7484 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2592.4960 - reconstruction_loss: 2586.8110 - kl_loss: 5.8827 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2592.1529 - reconstruction_loss: 2585.8186 - kl_loss: 5.8807 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2591.2567 - reconstruction_loss: 2585.1440 - kl_loss: 5.9121 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2590.6198 - reconstruction_loss: 2584.6377 - kl_loss: 5.9744 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2589.9608 - reconstruction_loss: 2584.6633 - kl_loss: 5.9996 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2589.3925 - reconstruction_loss: 2582.7397 - kl_loss: 6.0976 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2588.6987 - reconstruction_loss: 2582.3518 - kl_loss: 6.0944 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2588.5546 - reconstruction_loss: 2581.8818 - kl_loss: 6.1587 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2587.5672 - reconstruction_loss: 2581.4358 - kl_loss: 6.1752 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2586.8763 - reconstruction_loss: 2580.4463 - kl_loss: 6.2191 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2587.8927 - reconstruction_loss: 2580.8672 - kl_loss: 6.2035 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2585.8507 - reconstruction_loss: 2579.4160 - kl_loss: 6.2922 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 7s 116ms/step - loss: 2585.6049 - reconstruction_loss: 2579.2356 - kl_loss: 6.2922 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 7s 116ms/step - loss: 2585.6463 - reconstruction_loss: 2578.8230 - kl_loss: 6.3159 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 8s 117ms/step - loss: 2584.8282 - reconstruction_loss: 2578.7354 - kl_loss: 6.3222 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 8s 118ms/step - loss: 2583.8593 - reconstruction_loss: 2577.6328 - kl_loss: 6.4053 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 8s 120ms/step - loss: 2583.9013 - reconstruction_loss: 2577.9763 - kl_loss: 6.3555 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 8s 120ms/step - loss: 2584.7990 - reconstruction_loss: 2577.8237 - kl_loss: 6.4121 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 2584.2521 - reconstruction_loss: 2577.0583 - kl_loss: 6.4346 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 8s 120ms/step - loss: 2583.6037 - reconstruction_loss: 2576.8596 - kl_loss: 6.4836 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 2583.4937 - reconstruction_loss: 2577.4724 - kl_loss: 6.4225 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 8s 120ms/step - loss: 2582.6467 - reconstruction_loss: 2576.2979 - kl_loss: 6.4572 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 2583.1104 - reconstruction_loss: 2576.2627 - kl_loss: 6.4928 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 8s 122ms/step - loss: 2581.5701 - reconstruction_loss: 2575.3530 - kl_loss: 6.5278 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2581.5402 - reconstruction_loss: 2575.2410 - kl_loss: 6.5721 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2582.0979 - reconstruction_loss: 2575.0176 - kl_loss: 6.5472 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2581.3953 - reconstruction_loss: 2574.9646 - kl_loss: 6.5603 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2582.1442 - reconstruction_loss: 2575.4807 - kl_loss: 6.5377 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2582.0951 - reconstruction_loss: 2575.0381 - kl_loss: 6.5536 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2581.0900 - reconstruction_loss: 2574.4014 - kl_loss: 6.5647 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2580.6445 - reconstruction_loss: 2574.3892 - kl_loss: 6.5910 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2581.1967 - reconstruction_loss: 2574.4141 - kl_loss: 6.5564 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 2580.4861 - reconstruction_loss: 2574.1218 - kl_loss: 6.6083 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2580.6756 - reconstruction_loss: 2573.9602 - kl_loss: 6.6430 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2581.4157 - reconstruction_loss: 2574.8540 - kl_loss: 6.5867 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 8s 124ms/step - loss: 2580.7976 - reconstruction_loss: 2573.6045 - kl_loss: 6.6597 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 8s 125ms/step - loss: 2579.9919 - reconstruction_loss: 2573.3132 - kl_loss: 6.6760 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2579.8770 - reconstruction_loss: 2573.4241 - kl_loss: 6.6430 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 2579.8671 - reconstruction_loss: 2573.1904 - kl_loss: 6.6469 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 2580.0712 - reconstruction_loss: 2573.3442 - kl_loss: 6.6831 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 8s 125ms/step - loss: 2579.0896 - reconstruction_loss: 2572.6992 - kl_loss: 6.6879 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 2579.1798 - reconstruction_loss: 2572.9709 - kl_loss: 6.6925 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2579.4812 - reconstruction_loss: 2572.6902 - kl_loss: 6.7170 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2579.0158 - reconstruction_loss: 2572.5491 - kl_loss: 6.7613 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2578.3153 - reconstruction_loss: 2572.3667 - kl_loss: 6.7453 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2579.8677 - reconstruction_loss: 2572.9138 - kl_loss: 6.7174 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2579.3948 - reconstruction_loss: 2572.4722 - kl_loss: 6.7372 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2579.0722 - reconstruction_loss: 2572.5840 - kl_loss: 6.7449 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2579.4526 - reconstruction_loss: 2572.4424 - kl_loss: 6.7007 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2579.2154 - reconstruction_loss: 2572.0210 - kl_loss: 6.7880 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2579.5225 - reconstruction_loss: 2572.0691 - kl_loss: 6.7657 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2578.5193 - reconstruction_loss: 2572.0381 - kl_loss: 6.7554 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 2578.7702 - reconstruction_loss: 2571.5801 - kl_loss: 6.8130 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2578.5897 - reconstruction_loss: 2571.7974 - kl_loss: 6.8026 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 2577.6295 - reconstruction_loss: 2571.5828 - kl_loss: 6.7732 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2578.5225 - reconstruction_loss: 2571.5027 - kl_loss: 6.7921 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2578.9513 - reconstruction_loss: 2571.9414 - kl_loss: 6.8138 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2577.9978 - reconstruction_loss: 2571.0674 - kl_loss: 6.8204 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2578.0836 - reconstruction_loss: 2570.9890 - kl_loss: 6.8208 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2577.6584 - reconstruction_loss: 2571.2737 - kl_loss: 6.8024 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2578.0473 - reconstruction_loss: 2571.1006 - kl_loss: 6.8638 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2578.2266 - reconstruction_loss: 2570.9167 - kl_loss: 6.8140 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2577.5865 - reconstruction_loss: 2570.9102 - kl_loss: 6.8378 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2577.1295 - reconstruction_loss: 2570.8606 - kl_loss: 6.8357 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2578.7063 - reconstruction_loss: 2571.1550 - kl_loss: 6.8567 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2577.2186 - reconstruction_loss: 2571.3232 - kl_loss: 6.8291 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2578.0379 - reconstruction_loss: 2571.0005 - kl_loss: 6.8220 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2578.3771 - reconstruction_loss: 2571.5002 - kl_loss: 6.8320 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2578.4118 - reconstruction_loss: 2571.0090 - kl_loss: 6.8551 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2577.4230 - reconstruction_loss: 2570.6270 - kl_loss: 6.8564 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2577.5780 - reconstruction_loss: 2570.3145 - kl_loss: 6.8432 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.9462 - reconstruction_loss: 2570.4773 - kl_loss: 6.9133 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2577.1470 - reconstruction_loss: 2570.2451 - kl_loss: 6.9051 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.9918 - reconstruction_loss: 2570.0320 - kl_loss: 6.8911 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2577.2456 - reconstruction_loss: 2570.0947 - kl_loss: 6.8953 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2577.0161 - reconstruction_loss: 2570.0303 - kl_loss: 6.8944 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2576.4989 - reconstruction_loss: 2570.0264 - kl_loss: 6.9046 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2577.4323 - reconstruction_loss: 2569.9058 - kl_loss: 6.9152 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2576.8893 - reconstruction_loss: 2569.8315 - kl_loss: 6.9207 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 2576.1093 - reconstruction_loss: 2569.6096 - kl_loss: 6.9076 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 8s 133ms/step - loss: 2576.4822 - reconstruction_loss: 2569.7261 - kl_loss: 6.9440 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2577.9579 - reconstruction_loss: 2570.5317 - kl_loss: 6.8709 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.3099 - reconstruction_loss: 2569.7036 - kl_loss: 6.9167 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.4298 - reconstruction_loss: 2569.9832 - kl_loss: 6.8979 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2577.0787 - reconstruction_loss: 2569.6221 - kl_loss: 6.9446 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2576.2293 - reconstruction_loss: 2569.4836 - kl_loss: 6.9408 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2576.0499 - reconstruction_loss: 2569.9958 - kl_loss: 6.9571 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2577.2560 - reconstruction_loss: 2570.1414 - kl_loss: 6.9230 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2577.0819 - reconstruction_loss: 2569.2954 - kl_loss: 6.9671 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2576.4748 - reconstruction_loss: 2569.4031 - kl_loss: 6.9327 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.9103 - reconstruction_loss: 2569.6616 - kl_loss: 6.9354 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.2071 - reconstruction_loss: 2569.4604 - kl_loss: 6.9556 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.6885 - reconstruction_loss: 2569.0344 - kl_loss: 6.9703 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 2576.5868 - reconstruction_loss: 2569.3918 - kl_loss: 6.9387 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.8031 - reconstruction_loss: 2569.3147 - kl_loss: 6.9640 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2576.8786 - reconstruction_loss: 2569.4568 - kl_loss: 6.9582 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2576.7506 - reconstruction_loss: 2569.0730 - kl_loss: 6.9905 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 8s 133ms/step - loss: 2575.8707 - reconstruction_loss: 2569.1001 - kl_loss: 7.0013 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2576.2868 - reconstruction_loss: 2569.0986 - kl_loss: 6.9685 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2575.8845 - reconstruction_loss: 2568.8093 - kl_loss: 7.0010 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2576.7599 - reconstruction_loss: 2569.9307 - kl_loss: 6.9538 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2576.6512 - reconstruction_loss: 2569.2307 - kl_loss: 6.9783 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2576.4862 - reconstruction_loss: 2569.1238 - kl_loss: 7.0159 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.1273 - reconstruction_loss: 2568.8811 - kl_loss: 7.0067 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.8360 - reconstruction_loss: 2569.2473 - kl_loss: 6.9893 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2575.4459 - reconstruction_loss: 2568.3496 - kl_loss: 7.0412 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2575.6143 - reconstruction_loss: 2568.8374 - kl_loss: 7.0185 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.6336 - reconstruction_loss: 2568.6907 - kl_loss: 7.0414 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.5696 - reconstruction_loss: 2568.2517 - kl_loss: 7.0728 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2575.0647 - reconstruction_loss: 2568.4883 - kl_loss: 7.0583 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2575.4401 - reconstruction_loss: 2568.7024 - kl_loss: 7.0463 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2575.3536 - reconstruction_loss: 2568.5576 - kl_loss: 7.0411 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2575.7737 - reconstruction_loss: 2568.5154 - kl_loss: 7.0477 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2575.4746 - reconstruction_loss: 2568.2268 - kl_loss: 7.0461 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 8s 133ms/step - loss: 2575.0284 - reconstruction_loss: 2568.3291 - kl_loss: 7.0425 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2574.3092 - reconstruction_loss: 2568.2161 - kl_loss: 7.0774 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.5026 - reconstruction_loss: 2568.4260 - kl_loss: 7.0504 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2574.9663 - reconstruction_loss: 2568.4048 - kl_loss: 7.0681 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2576.4883 - reconstruction_loss: 2568.3613 - kl_loss: 7.0777 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2575.2824 - reconstruction_loss: 2568.1797 - kl_loss: 7.0668 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 8s 133ms/step - loss: 2575.2398 - reconstruction_loss: 2568.4092 - kl_loss: 7.0349 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2575.5075 - reconstruction_loss: 2567.8752 - kl_loss: 7.0681 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.6543 - reconstruction_loss: 2567.9204 - kl_loss: 7.0852 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2575.2824 - reconstruction_loss: 2568.2676 - kl_loss: 7.0551 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.7110 - reconstruction_loss: 2567.9927 - kl_loss: 7.0739 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2574.6488 - reconstruction_loss: 2567.9490 - kl_loss: 7.0694 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2574.6947 - reconstruction_loss: 2567.9995 - kl_loss: 7.1020 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2575.9709 - reconstruction_loss: 2567.9878 - kl_loss: 7.0879 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2574.7955 - reconstruction_loss: 2567.9292 - kl_loss: 7.1205 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2575.8090 - reconstruction_loss: 2568.0903 - kl_loss: 7.0878 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.6143 - reconstruction_loss: 2567.8567 - kl_loss: 7.0829 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.6290 - reconstruction_loss: 2567.9480 - kl_loss: 7.0943 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.6217 - reconstruction_loss: 2567.7798 - kl_loss: 7.0864 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.0823 - reconstruction_loss: 2567.8555 - kl_loss: 7.0756 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2575.1469 - reconstruction_loss: 2567.5391 - kl_loss: 7.1451 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.2357 - reconstruction_loss: 2567.3691 - kl_loss: 7.1200 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.0036 - reconstruction_loss: 2567.3789 - kl_loss: 7.1293 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.3690 - reconstruction_loss: 2567.6843 - kl_loss: 7.1185 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.8960 - reconstruction_loss: 2567.4768 - kl_loss: 7.1284 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2574.7610 - reconstruction_loss: 2567.7266 - kl_loss: 7.0895 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2575.3772 - reconstruction_loss: 2567.6934 - kl_loss: 7.1015 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.6888 - reconstruction_loss: 2567.3718 - kl_loss: 7.1430 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2575.2431 - reconstruction_loss: 2567.4204 - kl_loss: 7.1277 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.7732 - reconstruction_loss: 2567.8137 - kl_loss: 7.1025 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.6999 - reconstruction_loss: 2567.5801 - kl_loss: 7.1222 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.2222 - reconstruction_loss: 2567.4983 - kl_loss: 7.1473 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2574.6841 - reconstruction_loss: 2567.5415 - kl_loss: 7.1376 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.5194 - reconstruction_loss: 2567.5156 - kl_loss: 7.1397 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2574.2130 - reconstruction_loss: 2567.1360 - kl_loss: 7.1670 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2574.4444 - reconstruction_loss: 2567.1160 - kl_loss: 7.1411 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2574.2553 - reconstruction_loss: 2567.3389 - kl_loss: 7.1846 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.2155 - reconstruction_loss: 2566.9578 - kl_loss: 7.1643 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.5421 - reconstruction_loss: 2567.0833 - kl_loss: 7.1405 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2574.4187 - reconstruction_loss: 2567.3958 - kl_loss: 7.1637 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2574.3665 - reconstruction_loss: 2567.3005 - kl_loss: 7.1732 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 9s 137ms/step - loss: 2573.7611 - reconstruction_loss: 2566.9956 - kl_loss: 7.1606 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2574.1258 - reconstruction_loss: 2567.0183 - kl_loss: 7.1694 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2574.0455 - reconstruction_loss: 2567.1992 - kl_loss: 7.1968 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.4431 - reconstruction_loss: 2567.4070 - kl_loss: 7.1494 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2574.0477 - reconstruction_loss: 2566.7498 - kl_loss: 7.1838 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.9155 - reconstruction_loss: 2566.8318 - kl_loss: 7.2044 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.9408 - reconstruction_loss: 2566.8979 - kl_loss: 7.1811 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2573.9966 - reconstruction_loss: 2566.8865 - kl_loss: 7.1737 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.8663 - reconstruction_loss: 2566.7507 - kl_loss: 7.1921 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2574.5235 - reconstruction_loss: 2566.8743 - kl_loss: 7.2063 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.9595 - reconstruction_loss: 2566.8328 - kl_loss: 7.2098 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.4546 - reconstruction_loss: 2566.7202 - kl_loss: 7.2037 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.7612 - reconstruction_loss: 2567.0183 - kl_loss: 7.2100 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.9056 - reconstruction_loss: 2566.7607 - kl_loss: 7.1874 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.1963 - reconstruction_loss: 2566.7708 - kl_loss: 7.1916 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.2348 - reconstruction_loss: 2566.6968 - kl_loss: 7.1962 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.4613 - reconstruction_loss: 2566.5627 - kl_loss: 7.2252 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2574.2500 - reconstruction_loss: 2567.0110 - kl_loss: 7.2022 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.0735 - reconstruction_loss: 2566.5269 - kl_loss: 7.2047 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.6985 - reconstruction_loss: 2567.2422 - kl_loss: 7.1890 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.3867 - reconstruction_loss: 2566.6658 - kl_loss: 7.2149 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2574.1010 - reconstruction_loss: 2566.7861 - kl_loss: 7.2040 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.2371 - reconstruction_loss: 2566.3206 - kl_loss: 7.2435 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.9322 - reconstruction_loss: 2566.5061 - kl_loss: 7.2194 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2574.0194 - reconstruction_loss: 2566.5227 - kl_loss: 7.2413 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.2447 - reconstruction_loss: 2566.3230 - kl_loss: 7.2154 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2572.8945 - reconstruction_loss: 2566.0051 - kl_loss: 7.2760 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.2304 - reconstruction_loss: 2566.3198 - kl_loss: 7.2473 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.2068 - reconstruction_loss: 2566.5872 - kl_loss: 7.2234 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.9942 - reconstruction_loss: 2566.7073 - kl_loss: 7.2521 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.8688 - reconstruction_loss: 2566.2300 - kl_loss: 7.2355 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2572.7828 - reconstruction_loss: 2566.0195 - kl_loss: 7.2455 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.1715 - reconstruction_loss: 2566.0830 - kl_loss: 7.2484 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.5679 - reconstruction_loss: 2566.1743 - kl_loss: 7.2763 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2573.6161 - reconstruction_loss: 2566.2476 - kl_loss: 7.2582 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.4622 - reconstruction_loss: 2565.9319 - kl_loss: 7.2522 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.4266 - reconstruction_loss: 2566.2295 - kl_loss: 7.2291 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.0023 - reconstruction_loss: 2565.9900 - kl_loss: 7.2621 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.7200 - reconstruction_loss: 2565.9666 - kl_loss: 7.2867 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.2291 - reconstruction_loss: 2565.9719 - kl_loss: 7.2637 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.2533 - reconstruction_loss: 2565.8938 - kl_loss: 7.2873 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.5007 - reconstruction_loss: 2565.9658 - kl_loss: 7.2588 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2574.3236 - reconstruction_loss: 2565.9238 - kl_loss: 7.2550 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.1880 - reconstruction_loss: 2566.0237 - kl_loss: 7.2846 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2573.0773 - reconstruction_loss: 2566.0657 - kl_loss: 7.2821 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.1941 - reconstruction_loss: 2566.0894 - kl_loss: 7.2575 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.0198 - reconstruction_loss: 2565.7449 - kl_loss: 7.2951 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.0960 - reconstruction_loss: 2565.7446 - kl_loss: 7.2881 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 9s 137ms/step - loss: 2573.1023 - reconstruction_loss: 2565.9275 - kl_loss: 7.3146 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2573.5904 - reconstruction_loss: 2566.1448 - kl_loss: 7.2554 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2573.3189 - reconstruction_loss: 2565.7422 - kl_loss: 7.3103 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 9s 137ms/step - loss: 2573.4411 - reconstruction_loss: 2565.6455 - kl_loss: 7.3139 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2573.3400 - reconstruction_loss: 2566.2393 - kl_loss: 7.2619 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2572.8056 - reconstruction_loss: 2565.8076 - kl_loss: 7.2894 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2572.7595 - reconstruction_loss: 2565.5947 - kl_loss: 7.3000 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.0355 - reconstruction_loss: 2565.5068 - kl_loss: 7.3034 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2573.6880 - reconstruction_loss: 2565.6128 - kl_loss: 7.2881 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 8s 133ms/step - loss: 2572.8905 - reconstruction_loss: 2565.7373 - kl_loss: 7.2894 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.8602 - reconstruction_loss: 2565.6355 - kl_loss: 7.3405 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2572.7113 - reconstruction_loss: 2565.7017 - kl_loss: 7.2981 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.2640 - reconstruction_loss: 2565.6514 - kl_loss: 7.3177 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2572.9478 - reconstruction_loss: 2565.8186 - kl_loss: 7.3472 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 2572.2244 - reconstruction_loss: 2565.3879 - kl_loss: 7.3215 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2572.5402 - reconstruction_loss: 2565.7932 - kl_loss: 7.3163 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2572.9374 - reconstruction_loss: 2565.5071 - kl_loss: 7.3509 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2572.7109 - reconstruction_loss: 2565.2798 - kl_loss: 7.3077 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2572.5511 - reconstruction_loss: 2565.5457 - kl_loss: 7.3244 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 9s 133ms/step - loss: 2572.7410 - reconstruction_loss: 2565.6748 - kl_loss: 7.3206 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 9s 137ms/step - loss: 2573.3105 - reconstruction_loss: 2565.3604 - kl_loss: 7.3386 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2573.1326 - reconstruction_loss: 2565.2998 - kl_loss: 7.3509 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.7850 - reconstruction_loss: 2565.2490 - kl_loss: 7.3279 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.7438 - reconstruction_loss: 2565.3047 - kl_loss: 7.3646 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.8970 - reconstruction_loss: 2565.5703 - kl_loss: 7.3344 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2572.7345 - reconstruction_loss: 2565.2834 - kl_loss: 7.3317 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 2572.0685 - reconstruction_loss: 2565.2405 - kl_loss: 7.3520 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 9s 137ms/step - loss: 2572.6134 - reconstruction_loss: 2565.4546 - kl_loss: 7.3576 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2573.0882 - reconstruction_loss: 2565.5906 - kl_loss: 7.3400 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.4247 - reconstruction_loss: 2565.0671 - kl_loss: 7.3791 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.4197 - reconstruction_loss: 2565.2671 - kl_loss: 7.3537 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.9307 - reconstruction_loss: 2565.2166 - kl_loss: 7.3679 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.9691 - reconstruction_loss: 2564.9041 - kl_loss: 7.3650 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 9s 135ms/step - loss: 2571.6631 - reconstruction_loss: 2565.2183 - kl_loss: 7.3618 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 2572.7722 - reconstruction_loss: 2565.2083 - kl_loss: 7.3638 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "\n",
    "epochs = 256\n",
    "\n",
    "i = MODEL_START\n",
    "for model in models:\n",
    "\n",
    "    history = igvae.run_model(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, name=MODEL_NAMES[i], plot=False)\n",
    "    \n",
    "    pd.DataFrame(history.history).to_csv(f'res/histories/{MODEL_NAMES[i]}.csv')\n",
    "\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
