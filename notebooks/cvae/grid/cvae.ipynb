{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:00:28.200366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.16.1\n",
      "Available GPUs:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:00:33.214251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.243624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.243683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../image_gen_dm')) # or the path to your source code\n",
    "sys.path.append(str(module_path))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "tfkl = tfk.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(TF_ENABLE_ONEDNN_OPTS)\n",
    "os.environ['TG_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import image_gen_vae as igvae\n",
    "import image_gen_vae.constants as consts\n",
    "\n",
    "print('Tensorflow Version:', tf.__version__)\n",
    "print(\"Available GPUs: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "ENCODER_HIDDEN_SIZES = [\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "    [1792, 448],\n",
    "]\n",
    "\n",
    "DECODER_HIDDEN_SIZES = [] # Construct from encoder hidden sizes in reverse\n",
    "\n",
    "for hidden_sizes in ENCODER_HIDDEN_SIZES:\n",
    "    DECODER_HIDDEN_SIZES.append(hidden_sizes[::-1])\n",
    "\n",
    "\n",
    "ENCODER_CONV_CONFIGS = [\n",
    "    [(112, 3, 2), (224, 3, 2), (448, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (224, 3, 2), (448, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (224, 3, 2), (576, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (224, 3, 2), (576, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "\n",
    "    [(112, 3, 2), (288, 3, 2), (448, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (288, 3, 2), (448, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (288, 3, 2), (576, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(112, 3, 2), (288, 3, 2), (576, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "\n",
    "    [(144, 3, 2), (224, 3, 2), (448, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (224, 3, 2), (448, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (224, 3, 2), (576, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (224, 3, 2), (576, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "\n",
    "    [(144, 3, 2), (288, 3, 2), (448, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (288, 3, 2), (448, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (288, 3, 2), (576, 5, 4), (896, 3, 2), (2048, 3, 2)],\n",
    "    [(144, 3, 2), (288, 3, 2), (576, 5, 4), (1152, 3, 2), (2048, 3, 2)],\n",
    "]\n",
    "\n",
    "DECODER_CONV_CONFIGS = [] # Construct from encoder conv configs in reverse\n",
    "\n",
    "for conv_configs in ENCODER_CONV_CONFIGS:\n",
    "    DECODER_CONV_CONFIGS.append(conv_configs[::-1])\n",
    "\n",
    "FLATTENED_SHAPES = [\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "    (1, 1, 2048),\n",
    "]\n",
    "\n",
    "FLATTENED_SIZES = [] # Construct from flattened shapes\n",
    "\n",
    "for shape in FLATTENED_SHAPES:\n",
    "    FLATTENED_SIZES.append(shape[0] * shape[1] * shape[2])\n",
    "\n",
    "LATENT_DIM = 2\n",
    "\n",
    "MODEL_NAMES = []\n",
    "\n",
    "for i in range(len(ENCODER_HIDDEN_SIZES)):\n",
    "    MODEL_NAMES.append(f'cvae{i+1+28}')\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "VAL_PERCENTAGE = 0.1\n",
    "\n",
    "# Models to run\n",
    "\n",
    "MODEL_START = 9 # Rasmus 0-7, Viktor 8-15\n",
    "MODEL_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:00:33.375714: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.375829: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.375890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.551065: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.551136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.551147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-29 17:00:33.551190: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-29 17:00:33.551222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images:  8100\n",
      "Evaluation Images:  900\n",
      "Training Images (post-duplication):  8100\n",
      "Validation Images (post-duplication):  900\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "\n",
    "train_ds, val_ds = igvae.utils.load_datasets(val_percentage=VAL_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "\n",
    "# This code is heavily based on the Keras Team's example on Convolutional VAEs with only minor modifications.\n",
    "# The original code can be found at: https://github.com/keras-team/keras-io/blob/master/examples/generative/vae.py\n",
    "\n",
    "# Author: [fchollet](https://twitter.com/fchollet)\n",
    "# Licensed under the Apache License 2.0. See LICENSE file in the root directory for more information.\n",
    "\n",
    "\n",
    "class Sampling(tfkl.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CVAE(tfk.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            input_data, target_data = data\n",
    "            z_mean, z_log_var, z = self.encoder(input_data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.losses.binary_crossentropy(target_data, reconstruction), \n",
    "                    axis=(1,2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, input_data, training=False):\n",
    "        _, _, z = self.encoder(input_data)\n",
    "        reconstructed_image = self.decoder(z)\n",
    "\n",
    "        return reconstructed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 38\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 32, 32, 144)          4032      ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 224)          290528    ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 448)            2509248   ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 1152)           4646016   ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 1, 1, 2048)           2123571   ['conv2d_3[0][0]']            \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1792)                 3671808   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 448)                  803264    ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 2)                    898       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 2)                    898       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n",
      "                                                                     'z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33162404 (126.50 MB)\n",
      "Trainable params: 33162404 (126.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 448)               1344      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1792)              804608    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2048)              3672064   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 2, 2, 1152)        21234816  \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 4, 4, 448)         4645312   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 16, 16, 224)       2509024   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 32, 144)       290448    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2D  (None, 64, 64, 3)         3891      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33161507 (126.50 MB)\n",
      "Trainable params: 33161507 (126.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(MODEL_START, MODEL_START + MODEL_COUNT):\n",
    "    print(f'Model {i+1+28}')\n",
    "\n",
    "    image_input = tfkl.Input(shape=consts.INPUT_SHAPE, name='image_input')\n",
    "    \n",
    "    x = image_input\n",
    "    for filters, kernel_size, strides in ENCODER_CONV_CONFIGS[i]:\n",
    "        x = tfkl.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='relu', padding='same')(x)\n",
    "\n",
    "    flattened_image = tfkl.Flatten()(x)\n",
    "\n",
    "    x = flattened_image\n",
    "    for layer_size in ENCODER_HIDDEN_SIZES[i]:\n",
    "        x = tfkl.Dense(layer_size, \n",
    "                    activation='relu',\n",
    "                    kernel_initializer='glorot_uniform')(x)\n",
    "        \n",
    "    z_mean = tfkl.Dense(LATENT_DIM, name='z_mean', kernel_initializer='glorot_uniform')(x)\n",
    "    z_log_var = tfkl.Dense(LATENT_DIM, name='z_log_var', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    encoder = tfk.Model(inputs=image_input, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    latent_inputs = tfkl.Input(shape=(LATENT_DIM,))\n",
    "\n",
    "    y = latent_inputs\n",
    "    for layer_size in DECODER_HIDDEN_SIZES[i]:\n",
    "        y = tfkl.Dense(layer_size, \n",
    "                       activation='relu',\n",
    "                       kernel_initializer='glorot_uniform')(y)\n",
    "        \n",
    "    reconstructed_flattened_image = tfkl.Dense(FLATTENED_SIZES[i], kernel_initializer='glorot_uniform', activation='relu')(y)\n",
    "    \n",
    "    y = tfkl.Reshape(FLATTENED_SHAPES[i])(reconstructed_flattened_image)\n",
    "   \n",
    "    j = 0\n",
    "    for filters, kernel_size, strides in DECODER_CONV_CONFIGS[i]:\n",
    "        if j == len(DECODER_CONV_CONFIGS[i]) - 1:\n",
    "            y = tfkl.Conv2DTranspose(filters=3, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='sigmoid', padding='same')(y)\n",
    "        else:\n",
    "            y = tfkl.Conv2DTranspose(filters=DECODER_CONV_CONFIGS[i][j + 1][0], kernel_size=(kernel_size, kernel_size), strides=(strides, strides), activation='relu', padding='same')(y)\n",
    "        j += 1\n",
    "\n",
    "    decoder = tfk.Model(inputs=latent_inputs, outputs=y, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    # Model\n",
    "        \n",
    "    model = CVAE(encoder, decoder)\n",
    "    model.compile(optimizer=tfk.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:00:52.108867: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "2024-04-29 17:00:53.862534: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714410054.693379   28172 service.cc:145] XLA service 0x7f7229f61a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714410054.693440   28172 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-04-29 17:00:54.704373: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1714410054.792541   28172 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 32s 180ms/step - loss: 2802.4062 - reconstruction_loss: 2751.6582 - kl_loss: 2.8223 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 6s 94ms/step - loss: 2691.9924 - reconstruction_loss: 2686.5298 - kl_loss: 3.1726 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2687.6699 - reconstruction_loss: 2684.5303 - kl_loss: 2.9102 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 6s 94ms/step - loss: 2686.4787 - reconstruction_loss: 2683.9529 - kl_loss: 2.6505 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2684.9527 - reconstruction_loss: 2679.9077 - kl_loss: 2.5255 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2661.5849 - reconstruction_loss: 2651.5217 - kl_loss: 3.3041 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2643.9452 - reconstruction_loss: 2637.0425 - kl_loss: 3.7018 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2629.5347 - reconstruction_loss: 2620.5540 - kl_loss: 4.2424 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2613.7364 - reconstruction_loss: 2607.3123 - kl_loss: 4.8033 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2608.8482 - reconstruction_loss: 2603.8650 - kl_loss: 4.9525 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 6s 94ms/step - loss: 2605.8989 - reconstruction_loss: 2601.1763 - kl_loss: 5.0755 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2605.6077 - reconstruction_loss: 2599.8274 - kl_loss: 5.0609 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 2603.4140 - reconstruction_loss: 2598.0718 - kl_loss: 5.2128 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 6s 96ms/step - loss: 2602.7183 - reconstruction_loss: 2596.2783 - kl_loss: 5.2663 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 6s 97ms/step - loss: 2599.8680 - reconstruction_loss: 2594.7063 - kl_loss: 5.3701 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 6s 99ms/step - loss: 2599.8491 - reconstruction_loss: 2593.4438 - kl_loss: 5.4711 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 6s 100ms/step - loss: 2597.8275 - reconstruction_loss: 2591.5867 - kl_loss: 5.5553 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 6s 100ms/step - loss: 2596.4881 - reconstruction_loss: 2590.4585 - kl_loss: 5.6227 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 6s 101ms/step - loss: 2594.9966 - reconstruction_loss: 2588.8364 - kl_loss: 5.6937 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 7s 103ms/step - loss: 2594.5497 - reconstruction_loss: 2588.3997 - kl_loss: 5.7285 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 7s 103ms/step - loss: 2592.3799 - reconstruction_loss: 2586.8091 - kl_loss: 5.7923 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 7s 102ms/step - loss: 2591.5470 - reconstruction_loss: 2585.6311 - kl_loss: 5.8920 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 7s 104ms/step - loss: 2590.5033 - reconstruction_loss: 2585.2014 - kl_loss: 5.8982 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 7s 105ms/step - loss: 2591.0629 - reconstruction_loss: 2584.6042 - kl_loss: 5.8668 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 7s 105ms/step - loss: 2589.5465 - reconstruction_loss: 2583.7056 - kl_loss: 5.9753 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 7s 104ms/step - loss: 2588.7639 - reconstruction_loss: 2582.8040 - kl_loss: 6.0193 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 7s 103ms/step - loss: 2589.1158 - reconstruction_loss: 2582.5322 - kl_loss: 6.0312 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2588.5825 - reconstruction_loss: 2581.7974 - kl_loss: 6.0873 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2587.5051 - reconstruction_loss: 2581.5869 - kl_loss: 6.1012 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2588.1179 - reconstruction_loss: 2580.8464 - kl_loss: 6.1354 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 7s 105ms/step - loss: 2585.9021 - reconstruction_loss: 2580.2656 - kl_loss: 6.1390 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 7s 105ms/step - loss: 2586.1951 - reconstruction_loss: 2579.9702 - kl_loss: 6.1298 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2585.4130 - reconstruction_loss: 2579.1890 - kl_loss: 6.2041 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2584.6801 - reconstruction_loss: 2578.9900 - kl_loss: 6.2178 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2585.2792 - reconstruction_loss: 2578.8589 - kl_loss: 6.2591 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2584.5182 - reconstruction_loss: 2577.8618 - kl_loss: 6.2914 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2583.6274 - reconstruction_loss: 2577.7500 - kl_loss: 6.3047 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2584.6540 - reconstruction_loss: 2577.5735 - kl_loss: 6.2819 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2583.9957 - reconstruction_loss: 2577.3625 - kl_loss: 6.3505 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2583.2009 - reconstruction_loss: 2577.1428 - kl_loss: 6.3302 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2583.0532 - reconstruction_loss: 2576.8083 - kl_loss: 6.3740 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2582.9539 - reconstruction_loss: 2576.6226 - kl_loss: 6.4045 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 7s 105ms/step - loss: 2583.2765 - reconstruction_loss: 2576.4583 - kl_loss: 6.4191 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2581.6670 - reconstruction_loss: 2575.7346 - kl_loss: 6.4307 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2582.2067 - reconstruction_loss: 2575.8748 - kl_loss: 6.4214 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2582.2689 - reconstruction_loss: 2576.1443 - kl_loss: 6.3731 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2582.2097 - reconstruction_loss: 2575.6018 - kl_loss: 6.4432 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2582.3995 - reconstruction_loss: 2575.4028 - kl_loss: 6.4665 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2581.7882 - reconstruction_loss: 2574.9114 - kl_loss: 6.5022 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2581.2832 - reconstruction_loss: 2574.7651 - kl_loss: 6.5016 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2580.7857 - reconstruction_loss: 2574.8042 - kl_loss: 6.4863 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2581.7908 - reconstruction_loss: 2574.6631 - kl_loss: 6.5119 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2581.1085 - reconstruction_loss: 2574.3840 - kl_loss: 6.4979 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2580.9677 - reconstruction_loss: 2574.6089 - kl_loss: 6.5157 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2580.9983 - reconstruction_loss: 2573.9126 - kl_loss: 6.5747 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2580.0907 - reconstruction_loss: 2573.6184 - kl_loss: 6.5615 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2580.5151 - reconstruction_loss: 2573.7947 - kl_loss: 6.5571 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2580.3630 - reconstruction_loss: 2573.9875 - kl_loss: 6.5481 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2580.3792 - reconstruction_loss: 2573.5613 - kl_loss: 6.5564 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2580.2399 - reconstruction_loss: 2573.1714 - kl_loss: 6.5979 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2579.4021 - reconstruction_loss: 2573.3638 - kl_loss: 6.6118 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2579.8525 - reconstruction_loss: 2573.2800 - kl_loss: 6.5906 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2580.0943 - reconstruction_loss: 2573.1733 - kl_loss: 6.5880 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 7s 104ms/step - loss: 2579.2893 - reconstruction_loss: 2572.7068 - kl_loss: 6.6342 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2580.0139 - reconstruction_loss: 2573.1016 - kl_loss: 6.6550 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2580.8561 - reconstruction_loss: 2572.8926 - kl_loss: 6.6347 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2578.9386 - reconstruction_loss: 2572.6685 - kl_loss: 6.6757 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2579.1913 - reconstruction_loss: 2572.1663 - kl_loss: 6.6560 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2578.5743 - reconstruction_loss: 2572.1631 - kl_loss: 6.6873 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2579.7929 - reconstruction_loss: 2572.5901 - kl_loss: 6.6395 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2580.0962 - reconstruction_loss: 2572.7886 - kl_loss: 6.6625 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2579.1101 - reconstruction_loss: 2572.1272 - kl_loss: 6.7178 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2579.9647 - reconstruction_loss: 2572.0007 - kl_loss: 6.6998 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2578.7897 - reconstruction_loss: 2572.0386 - kl_loss: 6.7308 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2578.6479 - reconstruction_loss: 2571.5234 - kl_loss: 6.7346 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2578.1842 - reconstruction_loss: 2571.7026 - kl_loss: 6.7314 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2578.5959 - reconstruction_loss: 2572.3352 - kl_loss: 6.6783 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2578.5251 - reconstruction_loss: 2571.4490 - kl_loss: 6.7308 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2578.2151 - reconstruction_loss: 2571.4705 - kl_loss: 6.7460 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2578.5983 - reconstruction_loss: 2571.5303 - kl_loss: 6.7612 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2578.0553 - reconstruction_loss: 2571.1106 - kl_loss: 6.7554 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2577.3623 - reconstruction_loss: 2571.0552 - kl_loss: 6.7580 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2578.5109 - reconstruction_loss: 2571.3086 - kl_loss: 6.7427 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2578.0896 - reconstruction_loss: 2571.1453 - kl_loss: 6.7513 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2577.7907 - reconstruction_loss: 2570.9998 - kl_loss: 6.7986 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2577.1918 - reconstruction_loss: 2570.9075 - kl_loss: 6.7759 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2577.2609 - reconstruction_loss: 2570.8423 - kl_loss: 6.7848 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2577.1020 - reconstruction_loss: 2570.7898 - kl_loss: 6.7737 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2577.3454 - reconstruction_loss: 2570.5979 - kl_loss: 6.8138 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2577.4703 - reconstruction_loss: 2570.7024 - kl_loss: 6.8338 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2577.0085 - reconstruction_loss: 2570.6990 - kl_loss: 6.8049 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2577.2195 - reconstruction_loss: 2570.5037 - kl_loss: 6.8159 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2577.9543 - reconstruction_loss: 2570.3384 - kl_loss: 6.8077 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2577.1722 - reconstruction_loss: 2570.4814 - kl_loss: 6.8209 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2577.7185 - reconstruction_loss: 2570.2839 - kl_loss: 6.7997 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2577.9441 - reconstruction_loss: 2570.4038 - kl_loss: 6.8170 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2576.8401 - reconstruction_loss: 2570.0554 - kl_loss: 6.8071 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2577.9667 - reconstruction_loss: 2570.4341 - kl_loss: 6.8382 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2577.4804 - reconstruction_loss: 2570.2866 - kl_loss: 6.8419 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2577.1214 - reconstruction_loss: 2570.2622 - kl_loss: 6.8248 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2577.4444 - reconstruction_loss: 2570.1697 - kl_loss: 6.8470 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2576.8489 - reconstruction_loss: 2569.8455 - kl_loss: 6.8583 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2577.4147 - reconstruction_loss: 2570.0969 - kl_loss: 6.8254 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2576.3194 - reconstruction_loss: 2569.6936 - kl_loss: 6.8697 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2576.8927 - reconstruction_loss: 2570.1753 - kl_loss: 6.8670 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2576.4584 - reconstruction_loss: 2570.0154 - kl_loss: 6.8414 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2576.8037 - reconstruction_loss: 2569.5635 - kl_loss: 6.8669 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2576.7202 - reconstruction_loss: 2569.7466 - kl_loss: 6.8889 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2576.3009 - reconstruction_loss: 2569.5967 - kl_loss: 6.8653 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2577.2032 - reconstruction_loss: 2569.7122 - kl_loss: 6.9078 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2577.4039 - reconstruction_loss: 2569.7925 - kl_loss: 6.8699 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2576.2547 - reconstruction_loss: 2569.3511 - kl_loss: 6.8854 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2577.4221 - reconstruction_loss: 2569.9875 - kl_loss: 6.9012 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2575.6572 - reconstruction_loss: 2569.2546 - kl_loss: 6.9183 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2576.6146 - reconstruction_loss: 2569.4055 - kl_loss: 6.8958 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2576.4793 - reconstruction_loss: 2569.6035 - kl_loss: 6.9121 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2576.4425 - reconstruction_loss: 2569.3901 - kl_loss: 6.9375 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2575.9954 - reconstruction_loss: 2569.3738 - kl_loss: 6.9069 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2576.3366 - reconstruction_loss: 2569.1897 - kl_loss: 6.9056 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2576.1334 - reconstruction_loss: 2569.1057 - kl_loss: 6.9209 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.9731 - reconstruction_loss: 2569.1965 - kl_loss: 6.9186 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2576.2741 - reconstruction_loss: 2568.9136 - kl_loss: 6.9160 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2577.1228 - reconstruction_loss: 2569.2803 - kl_loss: 6.9025 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2576.1471 - reconstruction_loss: 2569.1985 - kl_loss: 6.9369 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.3754 - reconstruction_loss: 2568.9001 - kl_loss: 6.9119 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2576.1263 - reconstruction_loss: 2568.8274 - kl_loss: 6.9505 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2575.0914 - reconstruction_loss: 2569.0530 - kl_loss: 6.9083 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2575.4339 - reconstruction_loss: 2568.9690 - kl_loss: 6.9056 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2575.8598 - reconstruction_loss: 2568.7715 - kl_loss: 6.9284 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2576.2968 - reconstruction_loss: 2568.6296 - kl_loss: 6.9463 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.7101 - reconstruction_loss: 2568.8242 - kl_loss: 6.9780 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2576.1625 - reconstruction_loss: 2568.9614 - kl_loss: 6.9342 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.7581 - reconstruction_loss: 2568.8909 - kl_loss: 6.9360 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.8728 - reconstruction_loss: 2568.9360 - kl_loss: 6.9518 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2575.0513 - reconstruction_loss: 2568.4490 - kl_loss: 6.9729 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2575.6982 - reconstruction_loss: 2568.9819 - kl_loss: 6.9691 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.9336 - reconstruction_loss: 2568.4629 - kl_loss: 6.9729 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2575.6859 - reconstruction_loss: 2568.5618 - kl_loss: 6.9816 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.4654 - reconstruction_loss: 2568.4661 - kl_loss: 6.9624 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.7669 - reconstruction_loss: 2568.2307 - kl_loss: 6.9902 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2575.8219 - reconstruction_loss: 2568.7227 - kl_loss: 6.9491 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.7688 - reconstruction_loss: 2568.8616 - kl_loss: 6.9792 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.1519 - reconstruction_loss: 2568.6755 - kl_loss: 7.0100 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2575.2046 - reconstruction_loss: 2568.2163 - kl_loss: 7.0103 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2574.9550 - reconstruction_loss: 2568.1396 - kl_loss: 6.9875 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.1563 - reconstruction_loss: 2568.2195 - kl_loss: 6.9704 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2574.8984 - reconstruction_loss: 2568.1052 - kl_loss: 6.9725 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2575.6925 - reconstruction_loss: 2568.0261 - kl_loss: 7.0154 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2574.8542 - reconstruction_loss: 2567.9968 - kl_loss: 7.0036 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.0038 - reconstruction_loss: 2568.3337 - kl_loss: 7.0152 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2575.0504 - reconstruction_loss: 2568.0562 - kl_loss: 6.9977 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2575.6115 - reconstruction_loss: 2567.9656 - kl_loss: 7.0282 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.0176 - reconstruction_loss: 2568.0977 - kl_loss: 7.0143 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.2172 - reconstruction_loss: 2567.8843 - kl_loss: 6.9968 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.1201 - reconstruction_loss: 2568.0332 - kl_loss: 7.0395 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2575.6552 - reconstruction_loss: 2567.9990 - kl_loss: 6.9961 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.2075 - reconstruction_loss: 2567.8567 - kl_loss: 7.0197 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.8593 - reconstruction_loss: 2567.8008 - kl_loss: 7.0129 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2575.5580 - reconstruction_loss: 2568.2974 - kl_loss: 6.9984 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2574.9461 - reconstruction_loss: 2568.2637 - kl_loss: 7.0061 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2574.5967 - reconstruction_loss: 2567.8386 - kl_loss: 7.0461 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.6169 - reconstruction_loss: 2568.0862 - kl_loss: 7.0286 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2574.5729 - reconstruction_loss: 2567.7524 - kl_loss: 7.0312 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2575.8318 - reconstruction_loss: 2568.6870 - kl_loss: 6.9758 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.4757 - reconstruction_loss: 2567.6450 - kl_loss: 7.0546 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2574.4729 - reconstruction_loss: 2567.6094 - kl_loss: 7.0303 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.4791 - reconstruction_loss: 2567.5854 - kl_loss: 7.0132 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.3555 - reconstruction_loss: 2567.5505 - kl_loss: 7.0642 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.7457 - reconstruction_loss: 2567.3257 - kl_loss: 7.0405 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.4758 - reconstruction_loss: 2567.8198 - kl_loss: 7.0394 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2575.0221 - reconstruction_loss: 2567.6040 - kl_loss: 7.0569 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2574.8227 - reconstruction_loss: 2567.5273 - kl_loss: 7.0188 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2575.4549 - reconstruction_loss: 2567.8008 - kl_loss: 7.0188 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2574.1673 - reconstruction_loss: 2567.6362 - kl_loss: 7.0498 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2574.7650 - reconstruction_loss: 2567.6470 - kl_loss: 7.0391 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2575.1367 - reconstruction_loss: 2567.5676 - kl_loss: 7.0402 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.9542 - reconstruction_loss: 2567.3005 - kl_loss: 7.0452 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2574.8601 - reconstruction_loss: 2567.1299 - kl_loss: 7.0949 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2573.7375 - reconstruction_loss: 2567.2578 - kl_loss: 7.0643 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2574.7899 - reconstruction_loss: 2567.2964 - kl_loss: 7.0722 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2574.1377 - reconstruction_loss: 2567.4111 - kl_loss: 7.0419 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.7227 - reconstruction_loss: 2567.5215 - kl_loss: 7.0552 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2574.6685 - reconstruction_loss: 2566.9907 - kl_loss: 7.0907 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.6710 - reconstruction_loss: 2567.0239 - kl_loss: 7.0597 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2575.2958 - reconstruction_loss: 2567.4229 - kl_loss: 7.0882 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2574.5732 - reconstruction_loss: 2567.4319 - kl_loss: 7.0642 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2574.0932 - reconstruction_loss: 2567.1968 - kl_loss: 7.1092 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.8969 - reconstruction_loss: 2567.3953 - kl_loss: 7.1062 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2574.2494 - reconstruction_loss: 2567.1265 - kl_loss: 7.0814 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.9399 - reconstruction_loss: 2566.8762 - kl_loss: 7.0786 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2574.3156 - reconstruction_loss: 2566.8962 - kl_loss: 7.1377 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.9562 - reconstruction_loss: 2567.0166 - kl_loss: 7.0860 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2574.7415 - reconstruction_loss: 2567.2649 - kl_loss: 7.1033 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.6991 - reconstruction_loss: 2567.1465 - kl_loss: 7.1030 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.1156 - reconstruction_loss: 2566.9014 - kl_loss: 7.1015 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.8834 - reconstruction_loss: 2566.8423 - kl_loss: 7.1091 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2573.9763 - reconstruction_loss: 2566.8872 - kl_loss: 7.1011 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2574.2924 - reconstruction_loss: 2567.0476 - kl_loss: 7.1555 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2574.3543 - reconstruction_loss: 2567.4192 - kl_loss: 7.1116 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2574.0863 - reconstruction_loss: 2566.7549 - kl_loss: 7.0955 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2574.4402 - reconstruction_loss: 2566.5413 - kl_loss: 7.1140 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2574.6421 - reconstruction_loss: 2566.7490 - kl_loss: 7.1575 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2574.1891 - reconstruction_loss: 2566.9468 - kl_loss: 7.1188 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2574.0688 - reconstruction_loss: 2566.6042 - kl_loss: 7.1413 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2573.5847 - reconstruction_loss: 2566.7014 - kl_loss: 7.1544 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2573.6507 - reconstruction_loss: 2566.6809 - kl_loss: 7.1346 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2574.1705 - reconstruction_loss: 2566.8679 - kl_loss: 7.1378 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 7s 116ms/step - loss: 2573.5579 - reconstruction_loss: 2566.6482 - kl_loss: 7.1523 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.5850 - reconstruction_loss: 2566.4255 - kl_loss: 7.1301 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 7s 117ms/step - loss: 2573.4944 - reconstruction_loss: 2566.4292 - kl_loss: 7.1679 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2574.2902 - reconstruction_loss: 2566.7808 - kl_loss: 7.1458 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2574.0365 - reconstruction_loss: 2566.7227 - kl_loss: 7.1754 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.4164 - reconstruction_loss: 2566.3862 - kl_loss: 7.1627 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2573.9390 - reconstruction_loss: 2566.7664 - kl_loss: 7.1172 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2573.8824 - reconstruction_loss: 2566.7173 - kl_loss: 7.1344 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.2638 - reconstruction_loss: 2566.5098 - kl_loss: 7.1722 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2573.5413 - reconstruction_loss: 2566.4614 - kl_loss: 7.1626 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.1130 - reconstruction_loss: 2566.5166 - kl_loss: 7.1562 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2573.3481 - reconstruction_loss: 2566.2949 - kl_loss: 7.1553 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2573.3610 - reconstruction_loss: 2566.0676 - kl_loss: 7.1565 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.9007 - reconstruction_loss: 2566.6360 - kl_loss: 7.1839 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2572.9519 - reconstruction_loss: 2566.3245 - kl_loss: 7.1708 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2572.9920 - reconstruction_loss: 2566.4294 - kl_loss: 7.1489 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.5076 - reconstruction_loss: 2566.1768 - kl_loss: 7.2054 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.2905 - reconstruction_loss: 2565.8628 - kl_loss: 7.1666 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2572.3102 - reconstruction_loss: 2565.9944 - kl_loss: 7.1763 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2573.5444 - reconstruction_loss: 2566.2375 - kl_loss: 7.1870 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.7393 - reconstruction_loss: 2566.2290 - kl_loss: 7.1888 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2572.9905 - reconstruction_loss: 2566.0217 - kl_loss: 7.1878 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.5406 - reconstruction_loss: 2565.9438 - kl_loss: 7.2037 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 7s 115ms/step - loss: 2573.1861 - reconstruction_loss: 2565.9893 - kl_loss: 7.2025 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.8599 - reconstruction_loss: 2566.0000 - kl_loss: 7.2093 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2572.8504 - reconstruction_loss: 2566.2188 - kl_loss: 7.1732 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2573.1791 - reconstruction_loss: 2566.1208 - kl_loss: 7.1940 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 8s 119ms/step - loss: 2573.9072 - reconstruction_loss: 2566.0815 - kl_loss: 7.1871 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2572.5358 - reconstruction_loss: 2565.8774 - kl_loss: 7.2208 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.1642 - reconstruction_loss: 2565.6404 - kl_loss: 7.2297 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.2108 - reconstruction_loss: 2565.9292 - kl_loss: 7.2091 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 7s 112ms/step - loss: 2573.4661 - reconstruction_loss: 2565.7690 - kl_loss: 7.2105 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2572.6466 - reconstruction_loss: 2565.7832 - kl_loss: 7.1978 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2573.0644 - reconstruction_loss: 2565.6479 - kl_loss: 7.2259 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2573.2248 - reconstruction_loss: 2565.6770 - kl_loss: 7.2164 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2572.3354 - reconstruction_loss: 2565.7644 - kl_loss: 7.2165 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.1853 - reconstruction_loss: 2565.7275 - kl_loss: 7.1801 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2572.8215 - reconstruction_loss: 2565.7373 - kl_loss: 7.2297 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2572.7492 - reconstruction_loss: 2565.6777 - kl_loss: 7.1964 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2573.0545 - reconstruction_loss: 2565.8533 - kl_loss: 7.2345 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2573.5114 - reconstruction_loss: 2565.9731 - kl_loss: 7.2129 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 7s 114ms/step - loss: 2572.8389 - reconstruction_loss: 2566.0120 - kl_loss: 7.2341 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2572.7814 - reconstruction_loss: 2565.7109 - kl_loss: 7.2267 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2572.9949 - reconstruction_loss: 2565.6438 - kl_loss: 7.2578 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 2572.8642 - reconstruction_loss: 2565.5686 - kl_loss: 7.2215 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 2573.6062 - reconstruction_loss: 2565.4023 - kl_loss: 7.2720 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2572.2983 - reconstruction_loss: 2565.6494 - kl_loss: 7.2395 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 7s 111ms/step - loss: 2573.2932 - reconstruction_loss: 2565.5037 - kl_loss: 7.2454 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2573.3446 - reconstruction_loss: 2565.4014 - kl_loss: 7.2525 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "\n",
    "epochs = 256\n",
    "\n",
    "i = MODEL_START\n",
    "for model in models:\n",
    "\n",
    "    history = igvae.run_model(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, name=MODEL_NAMES[i], plot=False)\n",
    "    \n",
    "    pd.DataFrame(history.history).to_csv(f'res/histories/{MODEL_NAMES[i]}.csv')\n",
    "\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
